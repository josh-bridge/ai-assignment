\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage[top=3.5cm,left=2.7cm,right=2.7cm,bottom=3.5cm]{geometry} % 'showframe' to see borders
\usepackage{booktabs}

\graphicspath{ {diagrams/} }

\title{\vspace{2cm}6G6Z1705\\\textbf{Artificial Intelligence}\\\vspace{2cm}Scenario 2\\\vspace{2cm}}
\author{14032908\\Joshua Michael Ephraim Bridge\\joshua.m.bridge@stu.mmu.ac.uk\\\vspace{1cm}}

% \pagestyle{headings}

\begin{document}

\maketitle

\newpage

\doublespacing

\section{Introduction}
  In this report an AI classifier will be put forward which maps mamographical data to desired outputs (diagnoses). In order to do this two types of AI classifiers will be evaluated on their performance in this task, along with relevant pre-processing of the attributes to enhance classifier performance. The two classifier types will be a Decision Tree (J.48) and an Artificial Neural Network (Multilayer Perceptron, \cite{minsky2017perceptrons}). In order to evaluate their performance, considerations of both learning time \& classification accuracy will be taken into account.

\section{AI classifiers}
  In this section a brief study will be conducted into the two classifier types mentioned previously.

  \subsection{Decision Trees}
    A Decision Tree is a hierarchical variant of a multistage classifier \citep{safavian1991survey}. The tree structure itelf could be described as a single root node with 0 to many connected children, each themselves with 0 to many connected children. Any node in a decision tree with no children is known as a leaf node and will have a direct relationship with two or more class labels.
    % https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/
    % https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3

  % • A detailed description of decision tree parameters (individual research required) and how they might affect the performance of the classifier

  \subsection{Artificial Neural Networks}
  % • A brief description / introduction to artificial neural networks
  Multilayer Perceptron \citep{minsky2017perceptrons}.

  % • A detailed description of artificial neural network parameters (individual research required) and how they might affect the performance of the classifier

\section{Data set analysis}
  % • Description of the data set attributes (e.g.):
  %   (a) Distribution,
  %   (b) predictive, (individual research required)
  %   (c) Outliers,
  %   (d) Attribute measurement scales e.g. nominal, ratio etc.

\section{Classifier Prediction}
  % • Based on the evidence from sections 2 and 3, what are the strengths and weaknesses of a decision tree when applied to the dataset

  % • Based on the evidence from sections 2 and 3, what are the strengths and weaknesses of an artificial neural network when applied to the dataset

  % • Prediction: which classifier is likely to be best for the dataset (before you have done any experiments) Individual research required for this section.

\section{Initial Experiments}
  % Strategy for missing attribute values. Justified with evidence/argument

  % • Strategy for outliers, justified with evidence/argument

  % • Reported results for experiments to develop / test initial strategies (Individual student judgement to be used on how far to go with each type of classifier in experiments leave plenty of time for real experiments later)

\section{Main Experiments}

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Confidence (MO=2)}
    \begin{tabular}{c|r}
    \toprule
    \multicolumn{1}{l|}{Confidence} & \multicolumn{1}{l}{Accuracy} \\
    \midrule
    0.05  & 82.2 \\
    0.1   & 82.16 \\
    0.15  & 82.19 \\
    0.2   & 82.27 \\
    0.25  & 82.19 \\
    0.3   & \textbf{82.33} \\
    0.35  & 82.31 \\
    0.4   & 82.12 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Minimum number of objects highest classification 1 (C=0.3)}
    \begin{tabular}{c|r}
    \toprule
    \multicolumn{1}{l|}{Min Objects} & \multicolumn{1}{l}{Accuracy} \\
    \midrule
    2     & 82.33 \\
    5     & 82.3 \\
    10    & 82.24 \\
    15    & 82.54 \\
    18    & 82.83 \\
    19    & 82.99 \\
    20    & 83 \\
    21    & 83.04 \\
    22    & 82.98 \\
    30    & 83.16 \\
    35    & 83.53 \\
    40    & 83.73 \\
    45    & 83.8 \\
    50    & \textbf{83.82} \\
    60    & 83.04 \\
    70    & 82.82 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


\begin{table}[htbp]
  \centering
  \caption{Learning rate accuracy from 200-3000 epochs}
    \begin{tabular}{r|rrrrr}
    \toprule
    \multicolumn{1}{r}{} & \multicolumn{5}{|c}{Learning Rate} \\
    & 0.1   & 0.3   & 0.5   & 0.7   & 0.9 \\
    \midrule
        Epochs & \multicolumn{5}{|c}{Accuracy (\%)} \\
    \midrule
    200   & \textbf{81.19} & 80.72 & 80.49 & 80.21 & 79.93 \\
    250   & 80.19 & \textbf{80.99} & 80.58 & 80.39 & 79.97 \\
    350   & 81.27 & \textbf{81.36} & 81.01 & 80.5  & 80.2 \\
    450   & 81.49 & \textbf{81.52} & 80.87 & 80.55 & 80.39 \\
    550   & \textbf{81.72} & 81.56 & 81.13 & 81.08 & 80.74 \\
    650   & \textbf{82.05} & 81.7  & 81.34 & 81.27 & 80.92 \\
    750   & \textbf{82.04} & 81.68 & 81.59 & 81.37 & 81.14 \\
    850   & \textbf{82.16} & 81.77 & 81.76 & 81.52 & 81.16 \\
    950   & \textbf{82.12} & 81.9  & 81.71 & 81.66 & 81.32 \\
    1050  & \textbf{82.13} & 81.96 & 81.84 & 81.77 & 81.28 \\
    1150  & 81.99 & \textbf{82} & 81.8  & 81.81 & 81.34 \\
    1500  & 82.08 & \textbf{82.22} & 81.84 & 81.92 & 81.5 \\
    2000  & 82.23 & \textbf{82.32} & 81.88 & 81.94 & 81.74 \\
    3000  & \textbf{82.25} & 82.24 & 81.84 & 81.86 & 81.69 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\begin{table}[htbp]
  \centering
  \caption{Momentum accuracy from 200-3000 epochs (LR=0.4, HL=A)}
    \begin{tabular}{r|rrrrr}
    \toprule
          & \multicolumn{5}{c}{Momentum} \\
    \midrule
    \multicolumn{1}{l|}{Epochs} & 0.1   & 0.3   & 0.5   & 0.7   & 0.9 \\
    \midrule
    200   & 80.59 & \textbf{80.66} & 80.24 & 79.92 & 79.31 \\
    300   & \textbf{81.07} & 80.96 & 80.55 & 80.46 & 79.34 \\
    400   & \textbf{81.31} & 81.17 & 80.68 & 80.55 & 79.49 \\
    500   & 81.33 & \textbf{81.36} & 80.96 & 80.72 & 79.39 \\
    600   & 81.45 & \textbf{81.51} & 81.22 & 80.98 & 79.5 \\
    700   & 81.55 & \textbf{81.64} & 81.38 & 81.15 & 79.48 \\
    800   & 81.52 & \textbf{81.71} & 81.52 & 81.41 & 79.52 \\
    900   & 81.69 & \textbf{81.72} & 81.53 & 81.46 & 79.38 \\
    1000  & \textbf{81.85} & 81.78 & 81.55 & 81.42 & 79.46 \\
    1100  & 81.89 & \textbf{81.98} & 81.61 & 81.63 & 79.66 \\
    1500  & \textbf{82.07} & 82.04 & 81.69 & 81.61 & 79.9 \\
    2000  & \textbf{82.06} & 82.03 & 81.78 & 81.66 & 80.05 \\
    3000  & 81.98 & \textbf{82.05} & 81.81 & 81.64 & 80 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%



\begin{table}[htbp]
  \centering
  \caption{Two hidden layer ANN structure (LR=0.4, M=0.2, E=950)}
    \begin{tabular}{c|rrrrr}
      \toprule
        & \multicolumn{5}{c}{Second Layer Neurons} \\
      \midrule
        \multicolumn{1}{c|}{First Layer Neurons} & 1     & 2     & 3     & 4     & 5 \\
      \midrule
        1 & 82.34 & 82.25 & 82.43 & 82.64 & \textbf{82.67} \\
        2 & 81.78 & 81.89 & 82.29 & 82.06 & \textbf{82.38} \\
        3 & 81.02 & 81.32 & 81.79 & 81.96 & \textbf{82.01} \\
        4 & 80.66 & 81.38 & 80.92 & 80.99 & \textbf{81.07} \\
        5 & 80.55 & 80.53 & \textbf{81.29} & 81.02 & 80.7 \\
      \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


\begin{table}[htbp]
  \centering
  \caption{Learning rate impact on accuracy from 250-3000 epochs (M=0.2, HL=1)}
    \begin{tabular}{r|rrrrr}
    \toprule
          & \multicolumn{5}{c}{Learning Rate} \\
    \midrule
    \multicolumn{1}{l|}{Epochs} & 0.1   & 0.3   & 0.5   & 0.7   & 0.9 \\
    \midrule
    250   & 81.73 & \textbf{81.88} & 81.85 & 81.87 & 81.8 \\
    350   & \textbf{82.27} & 82.24 & 82.21 & 82.19 & 82.23 \\
    450   & \textbf{82.58} & 82.49 & 82.39 & 82.33 & 82.35 \\
    550   & 82.55 & \textbf{82.61} & 82.58 & 82.41 & 82.33 \\
    650   & \textbf{82.73} & 82.7  & 82.72 & 82.66 & 82.45 \\
    750   & 82.77 & 82.8  & \textbf{82.83} & 82.74 & 82.5 \\
    850   & 82.79 & \textbf{82.93} & 82.88 & 82.66 & 82.5 \\
    950   & 82.89 & \textbf{83} & 82.87 & 82.78 & 82.55 \\
    1050  & 82.9  & \textbf{83} & 82.89 & 82.83 & 82.61 \\
    1150  & 82.89 & \textbf{83} & 82.87 & 82.88 & 82.69 \\
    1500  & 82.98 & 82.99 & \textbf{83.09} & 83    & 82.87 \\
    2000  & 83.08 & 83.15 & \textbf{83.19} & 83.14 & 83 \\
    3000  & \textbf{83.25} & 83.19 & 83.21 & 83.18 & 83.01 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%



  % • Plan for DT experiments, with justification

  %   (a) Execution of DT experiments to find DT with highest Classification Accuracy

  %   (b) Execution of DT experiments to find the most highly pruned DT that does not have a significantly lower Classification Accuracy than the tree with best CA.

  % • Plan for artificial neural network experiments, with justification

  %   (a) Execution of artificial neural network experiments to find artificial neural network with highest classi- fication accuracy using a single layer of neurons

  %   (b) Execution of artificial neural network experiments to find artificial neural network with highest classi- fication accuracy using a multiple layers of neurons (Individual student judgement to be used on how many layers to be used).


\section{Advanced Pre-processing}
  % • Experiments with different methods of pre-processing the data (scaling, grouping, conversion etc.). This work could contribute marks for work reaching the 86%+ mark band). The section may be omitted if you do not anticipate reaching such a mark.

% http://yatani.jp/HCIstats/PCA
\section{Conclusions}
  % • A description of the best (pruned) decision tree for the dataset (including number of nodes, leaves, pruning parameters etc.).

  % • A diagram of the best pruned decision tree for the dataset.

  % • A description of the best artificial neural network for the dataset (numbers of neurons, layers etc.)

  % • A summary table of best classifiers

  % • A diagram of the best artificial neural network for the dataset.

  % • A statement of which performed best and therefore which you would recommend to the client to solve the task

  % • Short analysis or discussion of results

\newpage

\bibliographystyle{agsm}
\bibliography{report-jim}

\end{document}
