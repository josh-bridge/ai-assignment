\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage[top=3.5cm,left=2.7cm,right=2.7cm,bottom=3.5cm]{geometry} % 'showframe' to see borders
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}


\graphicspath{ {diagrams/} }

\title{\vspace{2cm}6G6Z1705\\\textbf{Artificial Intelligence}\\\vspace{2cm}Scenario 2\\\vspace{2cm}}
\author{14032908\\Joshua Michael Ephraim Bridge\\joshua.m.bridge@stu.mmu.ac.uk\\\vspace{1cm}}

% \pagestyle{headings}

\begin{document}

\maketitle

\newpage

\doublespacing

\section{Introduction}
  In this report an AI classifier will be put forward which maps mamographical data to desired outputs (diagnoses). In order to do this two types of AI classifiers will be evaluated on their performance in this task, along with relevant pre-processing of the attributes to enhance classifier performance. The two classifier types will be a Decision Tree (J.48) and an Artificial Neural Network (Multilayer Perceptron, \cite{minsky2017perceptrons}). In order to evaluate their performance, considerations of both learning time \& classification accuracy will be taken into account.

\section{AI classifiers}
  In this section a brief study will be conducted into the two classifier types mentioned previously.

  \subsection{Decision Trees}
    A decision tree is a type of classifier (specifically a hierarchical variant of a multistage classifier, as defined by \cite{safavian1991survey}) which uses a tree-like structure to test values on different attributes in a format similar to a flow chart. The tree structure itself could be described as a single root node with 0 to many connected children, each themselves with 0 to many connected children. Any node in a decision tree with no children is known as a leaf node and has a direct relationship with a class label. At each node in the tree a test is carried out on an attribute and the result of that test decides on which of the child nodes the process should continue onto. The process of completing each test from the root node to a leaf node should result in a classification of the data provided.

    Self-learning decision trees are often very useful because they explicitly define how the instances are classified within the tree, simplifying the process into a set of simple rules. This is different to ANN's (see section \ref{ann}) where the classification process is mostly hidden and can often be a very complex set of rules which would be very hard to follow.

    \subsubsection{C4.5 Decision tree}
      There are several parameters within the C4.5 algorithm that will affect the classification performance on a dataset.

      % \paragraph{ID3}
      % \begin{equation} \label{eq:entropy}
      %   H(X) = -\sum p(X)\log p(X)
      % \end{equation}
      %
      %
      % \begin{equation} \label{eq:informationgain}
      %   I(X,Y)= H(X)-H(X|Y)
      % \end{equation}

      \paragraph{Confidence.}
        The confidence parameter is a way of controlling the amount of error-based pruning \citep{quinlan1987simplifying} within the decision tree. More specifically, post-pruning is the process of estimating the error rate (probability of mis-classification) at each node in the tree, and deciding whether or not to remove the node. Lower values of the confidence factor will result in the post-pruning becoming much more aggressive with removing nodes \citep{beck2008backward}.

      \paragraph{Minimum number of objects.}
        Within weka the Minimum number of objects parameter controls the Minimum number of instances per leaf. This means that each leaf within the tree must have at least the specified amount of classified instances for it not to be pruned. This parameter is good for data-sets which are particularly noisy which could introduce some leaf nodes which are not very stable classifiers. With a higher minimum number of objects, the tree will likely become much more pruned.

    % https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/
    % https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3

  % • A detailed description of decision tree parameters (individual research required) and how they might affect the performance of the classifier

  \subsection{Artificial Neural Networks} \label{ann}
    An Artificial Neural Network is a mathematical system which is able to classify data by performing a series of mathematical functions (activation functions) which take weightings for each of their inputs and summise them into a single output. ANN's are designed in light of the way human/animal brains process information, via a series of neurons which are connected (in biology these connections are called synapses). Within an ANN each neuron is connected to either input attributes or the output of neruron(s) in another layer of the network. The connections between the neurons contain weightings which is the main principal behind how the network can emphasise some data over others.

    The neurons within an ANN can be split up into a series of ‘layers’, where the outputs from one layer of neurons will become the inputs for the next layer of neurons. Within a Multilayer Perceptron (see section \ref{mlp}) the layers consist of 1 input layer, 1 output layer, and at least 1 ‘hidden layer’ where each neuron in the hidden layer(s) and the output layer are neurons which perform an activation function.

    Within an ANN there must be a process of ‘learning’ which enables it to find the most optimal values for the weights which are used in the activation functions. This is done via backpropagation which enables the algorithm to modify the weights based on the error rate of the output, compared to the expected output.

    % Learning occurs in the perceptron by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation, a generalization of the least mean squares algorithm in the linear perceptron.

      \subsubsection{Multilayer Perceptron} \label{mlp}
        \citep{minsky2017perceptrons}
        \paragraph{Hidden Layers.}
          The hidden layers parameter allows the user to define the strcuture of the network they would like to train. Introducing more layers \& neurons introduces more complexity which is good for more complex datasets with attributes which are not lineraly seperable, however for simpler datasets this may introduce unwanted complexity within the network. What hidden layers are and how they relate to the ANN is explained in more deatail in section \ref{ann}.

        \paragraph{Learning Rate.}
          Learning rate applies to the backpropagation algorithm and more specifically the Gradient Descent. It concerns the speed at which the minimum squared error is reached. A low learning rate would mean that many updates (a high training time) would be needed in order to find the global minimum - which is not desirable. If the learning rate is too high however, then this can lead to divergent behaviour where the backpropagation algorithm is not able to correctly settle on an optimal minimum.

          % This algorithm aims at learning the weights of a multilayer ANN with a fixed number of units and interconnections, by using gradient descent in order to minimize the squared error between the network outputs and the correspondent target values.

          % The backpropagation algorithm can be viewed as a searching algorithm whose state space corresponds to all possible weights for the network units. The algorithm uses the squared error between the network outputs and the target values as a fitness function to guide the gradient descend. The correspondent error surface can thus be considered multidimensional and may have multiple local minima. This characteristic guarantees only that the gradient descent will converge to a local minima and not necessarily to the global minimum error.

        \paragraph{Momentum.}
          Once again momentum relates to the gradient descent for squared error, however momentum defines the way in which the minima is reached. As there may be several local minimas within the descent path, it would not be desirable to end in a minima which is not actually the global minima. In order to avoid this, the momentum value is linked to the learning rate in that increasing the momentum allows the descent path to continue past local minimas in search of lower squared errors.
          % A descent gradient is used in order to reach the global minima. However, as this is trying to be found, the algorithm may encounter a local minimum which it may think is the global minima. This can be avoided by changing the value of the momentum, this parameter is linked to the learning rate so as momentum increases, learning rate should remain a smaller value to avoid the global minima from being missed.

        \paragraph{Training Time.}
          The main factor in the learning process of an ANN is the amount of time it has to train. There is no point in time which the network will be ‘done’ learning therefore the most optimal amount of learning time must be chosen. Training time is measured in ‘epochs’, where 1 epoch is the completion of a single training iteration. A training iteration includes the inputs passing through every layer, providing an output, and then the backpropagation algorithm updating the weights and biases for each applicable neuron. If the training time is too high, this can lead to something called ‘overtraining’ where the ANN becomes too dependent on the training data and will start giving worse results when presented with unseen testing data. Therefore it is necessary to find the optimal training time, in combination with the optimal learning rate and momentum.

\section{Data set analysis} \label{data-set-analysis}
  In this section, the mamographical dataset will be analysed as a preparatory step to pre-processing of the data.

  % • Description of the data set attributes (e.g.):
  %   (a) Distribution,
  %   (b) predictive, (individual research required)
  %   (c) Outliers,
  %   (d) Attribute measurement scales e.g. nominal, ratio etc.


    \subsection{Distribution}
      Below are a set of graphs which show the distribution of the data in each attribute of the dataset. As found in figure \ref{fig:birads-outlier}, there is an outlier in the BiRads data, which in some of the below figures is replaced by the median.
  \newpage
    \begin{figure}[H]
      \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{birads-histogram}
        \caption{Initial.}
        \label{fig:birads-histogram}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{birads-histogram-no55}
        \caption{Outlier replaced with median.}
        \label{fig:birads-histogram-no55}
      \end{subfigure}
      \caption{BiRads histogram.}
    \end{figure}

    \begin{figure}[H]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{age-histogram}
        \caption{Age histogram.}
        \label{fig:age-histogram}
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{density-histogram}
        \caption{Density histogram.}
        \label{fig:density-histogram}
      \end{minipage}
    \end{figure}

    \begin{figure}[H]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{margin-histogram}
        \caption{Margin histogram.}
        \label{fig:margin-histogram}
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{shape-histogram}
        \caption{Shape histogram.}
        \label{fig:shape-histogram}
      \end{minipage}
    \end{figure}

    \begin{figure}[H]
      \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{birads-probability}
        \caption{Initial.}
        \label{fig:birads-probability}
      \end{subfigure}
      \hfill
      \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{birads-probability}
        \caption{Outlier replaced with median.}
        \label{fig:birads-probability}
      \end{subfigure}
      \caption{BiRads probability.}
    \end{figure}

    \begin{figure}[H]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{age-probability}
        \caption{Age probability.}
        \label{fig:age-probability}
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{density-probability}
        \caption{Density probability.}
        \label{fig:density-probability}
      \end{minipage}
    \end{figure}

    \begin{figure}[H]
      \centering
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{margin-probability}
        \caption{Margin probability.}
        \label{fig:margin-probability}
      \end{minipage}
      \hfill
      \begin{minipage}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{shape-probability}
        \caption{Shape probability.}
        \label{fig:shape-probability}
      \end{minipage}
    \end{figure}

      \begin{figure}[H]
        \begin{subfigure}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{birads-outlier-plot}
          \caption{Initial.}
          \label{fig:birads-outlier}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.45\textwidth}
          \includegraphics[width=\textwidth]{birads-outlier-plot-no55}
          \caption{Outlier replaced with median.}
          \label{fig:birads-outilier-no55}
        \end{subfigure}
        \caption{BiRads outlier plots.}
      \end{figure}

  \subsection{Attributes}
    \paragraph{BI-RADS.} \label{birad}
      The acronym BI-RADS stands for “Breast Imaging Reporting and Data System” \citep{american1998breast}. It is a system which was designed to introduce some standardisation into the field of diagnosing breast cancer. The score for BI-RADS is on an ordinal scale from 1 to 5, with 1 being benign and 5 being very likely malignant.
      Below are the 5 definitions of the BI-RADS scale as defined by \cite{american1998breast}.

      \singlespacing
      \begin{enumerate}[label=\arabic*)]
        \item Negative
        \item Benign findings
        \item Probably benign
        \item Suspicious abnormality
        \item Highly suggesting of malignancy
      \end{enumerate}
      \doublespacing

      With the BI-RADS score being a predictive scale, this would mean that if the score of BI-RADS was high then the chance of that instance having a melignant severity would be much higher. This can be proven in figure \ref{fig:birads-severity-correlation} where it is shown that there is a correlation between a higher BI-RADS score and \% of severity classification. Figure \ref{fig:birads-outilier-no55} shows that there are several instances with BI-RADS scores of both ‘0’ and ‘6’ which do not exist in the scale. These scores do exist in edition 4 of the BI-RADS scale \citep{d2003breast} however the data descriptors provided with the dataset make no mention of these categories or which edition of the BI-RADS scale it refers to.

      \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{birads-severity-correlation}
        \caption{BI-RADS - Severity correlation}
        \label{fig:birads-severity-correlation}
      \end{figure}

    \paragraph{Age.}
      This is the only non-predictive attribute provided in the dataset, and is a simple ratio scale of the patients age. While age is not a predictive attribute, women diagnosed with breast cancer are much more likely to be above 50 years of age \citep{kerlikowske1993mammography}. This can be backed up by figure \ref{fig:age-severity-correlation} which shows a clear correlation between age and the \% of severity classifications per age group in the provided dataset.

      \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{age-severity-correlation}
        \caption{Age - Severity correlation}
        \label{fig:age-severity-correlation}
      \end{figure}

      Due to this and the findings with the BI-RADS attribute, it could be inferred that those with an age above 50 and a high BI-RADS score have a very high change of having a malignant severity.

      As shown in figure \ref{fig:age-histogram}, the distribution of age within the dataset is not normal and is skewed very slightly right.

    \paragraph{Shape.}
      This attribute is used to describe the shape of the mass being investigated. It is a nominal attribute as the number scale has no bearing on its meaning. Below are the definitions for each of the shape scores as defined by the dataset.

      \singlespacing
      \begin{enumerate}[label=\arabic*)]
        \item Round
        \item Oval
        \item Lobular
        \item Irregular
      \end{enumerate}
      \doublespacing

      In figure \ref{fig:shape-severity-correlation} below it can be seen that while shape is not on an ordinal scale, a higher score of shape is correlated with a higher chance of a malignant severity.

      \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{shape-severity-correlation}
        \caption{Shape - Severity correlation}
        \label{fig:shape-severity-correlation}
      \end{figure}

    \paragraph{Margin.}
      The margin attribute is used to describe the edges of the mass and their distinctiveness from the rest of the breast tissue within the scan. This is also a nominal attribute as there is no scale for the margin descriptors. Below are the 5 different definitions for each of the margin scores.

      \singlespacing
      \begin{enumerate}[label=\arabic*)]
        \item Circumscribed
        \item Microlobulated
        \item Obscured
        \item Ill-defined
        \item Spiculated
      \end{enumerate}
      \doublespacing

      Once again, it can be observed in figure \ref{fig:margin-severity-correlation} that a higher score of margin will correlate with a higher chance of a malignant severity.

      \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{margin-severity-correlation}
        \caption{Margin - Severity correlation}
        \label{fig:margin-severity-correlation}
      \end{figure}

    \paragraph{Density.}
      This attribute is used to describe the density of the mass on an ordinal scale. A lower score indicates a higher density, which has been shown to be a significant predictor of breast cancer \citep{woods2011mammographic}. Below are the descriptors used to score masses onto the ordinal scale.

      \singlespacing
      \begin{enumerate}[label=\arabic*)]
        \item High
        \item ISO (Isodense)
        \item Low
        \item Fat-containing
      \end{enumerate}
      \doublespacing

      Figure \ref{fig:density-severity-correlation} below shows that in contrary to \cite{woods2011mammographic}, the dataset provided does not seem to have a significant correlation between high-density masses and a malignant severity.

      \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{density-severity-correlation}
        \caption{Density - Severity correlation}
        \label{fig:density-severity-correlation}
      \end{figure}

    \paragraph{Severity.}
      This field is known as a goal field and is a binomial type. This field contains the classification of the instances. In order to make the dataset more compatible with WEKA processing, the original values of ‘0’ and ‘1’ have been replaced by ‘$-$’ and ‘+’ respectively.

\section{Classifier Evaluation}
  \subsection{Decision Tree}

    % Should be faster once trained (although both algorithms can train slowly depending on exact algorithm and the amount/dimensionality of the data). This is because a decision tree inherently "throws away" the input features that it doesn't find useful, whereas a neural net will use them all unless you do some feature selection as a pre-processing step.
    % If it is important to understand what the model is doing, the trees are very interpretable.
    % Only model functions which are axis-parallel splits of the data, which may not be the case.
    % You probably want to be sure to prune the tree to avoid over-fitting.
    \subsubsection{Strengths}
      \paragraph{Speed.}
        Decision trees are often a lot faster than neural networks (once trained), this is due to the fact that the algorithm creates a set of simple rules which are very quick to execute, and can be implemented in any programming language.
      \paragraph{Interpretability.}
        Again due to the nature of decision trees, the resulting classifier is very easy to interpret due to the creation of simple rules. It then provides insights on the relationships between the attributes and the classes.
      \paragraph{Selectivity.}
        Decision trees are good with noisy data due to the fact that they are able to completely ignore certain attributes if they are not deemed important enough as a contributor to classification.

    \subsubsection{Weaknesses}
      \paragraph{Nonlinear interactions.}
        Decision trees are not as good at handling data with nonlinear relationships due to their structure. If the relationship between two attributes is not definable in a simple rule then a decision tree will not be able to

  \subsection{Artificial Neural Network}
    % Slower (both for training and classification), and less interpretable.
    % If your data arrives in a stream, you can do incremental updates with stochastic gradient descent (unlike decision trees, which use inherently batch-learning algorithms).
    % Can model more arbitrary functions (nonlinear interactions, etc.) and therefore might be more accurate, provided there is enough training data. But it can be prone to over-fitting as well.
    \subsubsection{Strengths}

    \subsubsection{Weaknesses}

  \subsection{Prediction}
    In order to predict the outcome of which classifier is most suited for this task, it must be made clear how the dataset will interact with the classifier.
    In section \ref{data-set-analysis} it was shown that the dataset has some clear links between attributes and classifications (such as a high correlation between age and severity shown in figure \ref{fig:age-severity-correlation}). Due to the nature of decision trees which are more able to deal with data that has a clear link between attributes and classification, it could be presumed that the decision tree classifier would be more suited for this task than an artificial neural network.

\section{Initial Experiments}
  % Strategy for missing attribute values. Justified with evidence/argument

  % • Strategy for outliers, justified with evidence/argument

  % • Reported results for experiments to develop / test initial strategies (Individual student judgement to be used on how far to go with each type of classifier in experiments leave plenty of time for real experiments later)

\section{Main Experiments}




% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Confidence (MO=2)}
    \begin{tabular}{c|r}
    \toprule
    \multicolumn{1}{l|}{Confidence} & \multicolumn{1}{l}{Accuracy} \\
    \midrule
    0.05  & 82.2 \\
    0.1   & 82.16 \\
    0.15  & 82.19 \\
    0.2   & 82.27 \\
    0.25  & 82.19 \\
    0.3   & \textbf{82.33} \\
    0.35  & 82.31 \\
    0.4   & 82.12 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Minimum number of objects highest classification 1 (C=0.3)}
    \begin{tabular}{c|r}
    \toprule
    \multicolumn{1}{l|}{Min Objects} & \multicolumn{1}{l}{Accuracy} \\
    \midrule
    2     & 82.33 \\
    5     & 82.3 \\
    10    & 82.24 \\
    15    & 82.54 \\
    18    & 82.83 \\
    19    & 82.99 \\
    20    & 83 \\
    21    & 83.04 \\
    22    & 82.98 \\
    30    & 83.16 \\
    35    & 83.53 \\
    40    & 83.73 \\
    45    & 83.8 \\
    50    & \textbf{83.82} \\
    60    & 83.04 \\
    70    & 82.82 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


\begin{table}[htbp]
  \centering
  \caption{Learning rate accuracy from 200-3000 epochs}
    \begin{tabular}{r|rrrrr}
    \toprule
    \multicolumn{1}{r}{} & \multicolumn{5}{|c}{Learning Rate} \\
    & 0.1   & 0.3   & 0.5   & 0.7   & 0.9 \\
    \midrule
        Epochs & \multicolumn{5}{|c}{Accuracy (\%)} \\
    \midrule
    200   & \textbf{81.19} & 80.72 & 80.49 & 80.21 & 79.93 \\
    250   & 80.19 & \textbf{80.99} & 80.58 & 80.39 & 79.97 \\
    350   & 81.27 & \textbf{81.36} & 81.01 & 80.5  & 80.2 \\
    450   & 81.49 & \textbf{81.52} & 80.87 & 80.55 & 80.39 \\
    550   & \textbf{81.72} & 81.56 & 81.13 & 81.08 & 80.74 \\
    650   & \textbf{82.05} & 81.7  & 81.34 & 81.27 & 80.92 \\
    750   & \textbf{82.04} & 81.68 & 81.59 & 81.37 & 81.14 \\
    850   & \textbf{82.16} & 81.77 & 81.76 & 81.52 & 81.16 \\
    950   & \textbf{82.12} & 81.9  & 81.71 & 81.66 & 81.32 \\
    1050  & \textbf{82.13} & 81.96 & 81.84 & 81.77 & 81.28 \\
    1150  & 81.99 & \textbf{82} & 81.8  & 81.81 & 81.34 \\
    1500  & 82.08 & \textbf{82.22} & 81.84 & 81.92 & 81.5 \\
    2000  & 82.23 & \textbf{82.32} & 81.88 & 81.94 & 81.74 \\
    3000  & \textbf{82.25} & 82.24 & 81.84 & 81.86 & 81.69 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

\begin{table}[htbp]
  \centering
  \caption{Momentum accuracy from 200-3000 epochs (LR=0.4, HL=A)}
    \begin{tabular}{r|rrrrr}
    \toprule
          & \multicolumn{5}{c}{Momentum} \\
    \midrule
    \multicolumn{1}{l|}{Epochs} & 0.1   & 0.3   & 0.5   & 0.7   & 0.9 \\
    \midrule
    200   & 80.59 & \textbf{80.66} & 80.24 & 79.92 & 79.31 \\
    300   & \textbf{81.07} & 80.96 & 80.55 & 80.46 & 79.34 \\
    400   & \textbf{81.31} & 81.17 & 80.68 & 80.55 & 79.49 \\
    500   & 81.33 & \textbf{81.36} & 80.96 & 80.72 & 79.39 \\
    600   & 81.45 & \textbf{81.51} & 81.22 & 80.98 & 79.5 \\
    700   & 81.55 & \textbf{81.64} & 81.38 & 81.15 & 79.48 \\
    800   & 81.52 & \textbf{81.71} & 81.52 & 81.41 & 79.52 \\
    900   & 81.69 & \textbf{81.72} & 81.53 & 81.46 & 79.38 \\
    1000  & \textbf{81.85} & 81.78 & 81.55 & 81.42 & 79.46 \\
    1100  & 81.89 & \textbf{81.98} & 81.61 & 81.63 & 79.66 \\
    1500  & \textbf{82.07} & 82.04 & 81.69 & 81.61 & 79.9 \\
    2000  & \textbf{82.06} & 82.03 & 81.78 & 81.66 & 80.05 \\
    3000  & 81.98 & \textbf{82.05} & 81.81 & 81.64 & 80 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%



\begin{table}[htbp]
  \centering
  \caption{Two hidden layer ANN structure (LR=0.4, M=0.2, E=950)}
    \begin{tabular}{c|rrrrr}
      \toprule
        & \multicolumn{5}{c}{Second Layer Neurons} \\
      \midrule
        \multicolumn{1}{c|}{First Layer Neurons} & 1     & 2     & 3     & 4     & 5 \\
      \midrule
        1 & 82.34 & 82.25 & 82.43 & 82.64 & \textbf{82.67} \\
        2 & 81.78 & 81.89 & 82.29 & 82.06 & \textbf{82.38} \\
        3 & 81.02 & 81.32 & 81.79 & 81.96 & \textbf{82.01} \\
        4 & 80.66 & 81.38 & 80.92 & 80.99 & \textbf{81.07} \\
        5 & 80.55 & 80.53 & \textbf{81.29} & 81.02 & 80.7 \\
      \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


\begin{table}[htbp]
  \centering
  \caption{Learning rate impact on accuracy from 250-3000 epochs (M=0.2, HL=1)}
    \begin{tabular}{r|rrrrr}
    \toprule
          & \multicolumn{5}{c}{Learning Rate} \\
    \midrule
    \multicolumn{1}{l|}{Epochs} & 0.1   & 0.3   & 0.5   & 0.7   & 0.9 \\
    \midrule
    250   & 81.73 & \textbf{81.88} & 81.85 & 81.87 & 81.8 \\
    350   & \textbf{82.27} & 82.24 & 82.21 & 82.19 & 82.23 \\
    450   & \textbf{82.58} & 82.49 & 82.39 & 82.33 & 82.35 \\
    550   & 82.55 & \textbf{82.61} & 82.58 & 82.41 & 82.33 \\
    650   & \textbf{82.73} & 82.7  & 82.72 & 82.66 & 82.45 \\
    750   & 82.77 & 82.8  & \textbf{82.83} & 82.74 & 82.5 \\
    850   & 82.79 & \textbf{82.93} & 82.88 & 82.66 & 82.5 \\
    950   & 82.89 & \textbf{83} & 82.87 & 82.78 & 82.55 \\
    1050  & 82.9  & \textbf{83} & 82.89 & 82.83 & 82.61 \\
    1150  & 82.89 & \textbf{83} & 82.87 & 82.88 & 82.69 \\
    1500  & 82.98 & 82.99 & \textbf{83.09} & 83    & 82.87 \\
    2000  & 83.08 & 83.15 & \textbf{83.19} & 83.14 & 83 \\
    3000  & \textbf{83.25} & 83.19 & 83.21 & 83.18 & 83.01 \\
    \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%



  % • Plan for DT experiments, with justification

  %   (a) Execution of DT experiments to find DT with highest Classification Accuracy

  %   (b) Execution of DT experiments to find the most highly pruned DT that does not have a significantly lower Classification Accuracy than the tree with best CA.

  % • Plan for artificial neural network experiments, with justification

  %   (a) Execution of artificial neural network experiments to find artificial neural network with highest classification accuracy using a single layer of neurons

  %   (b) Execution of artificial neural network experiments to find artificial neural network with highest classification accuracy using a multiple layers of neurons (Individual student judgement to be used on how many layers to be used).


\section{Advanced Pre-processing}
  % • Experiments with different methods of pre-processing the data (scaling, grouping, conversion etc.). This work could contribute marks for work reaching the 86%+ mark band). The section may be omitted if you do not anticipate reaching such a mark.

% http://yatani.jp/HCIstats/PCA
\section{Conclusions}
  % • A description of the best (pruned) decision tree for the dataset (including number of nodes, leaves, pruning parameters etc.).

  % • A diagram of the best pruned decision tree for the dataset.

  % • A description of the best artificial neural network for the dataset (numbers of neurons, layers etc.)

  % • A summary table of best classifiers

  % • A diagram of the best artificial neural network for the dataset.

  % • A statement of which performed best and therefore which you would recommend to the client to solve the task

  % • Short analysis or discussion of results

\newpage

\bibliographystyle{agsm}
\bibliography{report-jim}

\end{document}
